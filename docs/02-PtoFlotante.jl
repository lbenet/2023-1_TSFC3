# # N√∫meros en la computadora: n√∫meros de punto flotante

#-
#Preliminares
using Pkg
Pkg.activate("..")  # Activa el directorio "." respecto al lugar donde estamos

#-
# ## Ejemplos: sutilezas al usar la aritm√©tica de la computadora

# ### El orden de las operaciones importa

# Consideren los siguientes dos vectores $a$ y $b$

a = [1e20, 2.0, -1e22, 1e13, 2111, 1e16]

#-
b = [1e20, 1223, 1e18, 1e15, 3, -1e12]

# La pregunta es cu√°nto vale el producto punto de estos vectores, ``a\cdot b``.

#-
# Primero usaremos la manera m√°s directa, que es el c√°lculo de la suma de productos elemento a elemento de los vectores; para obtener el producto elemento a element usaremos *broadcasting* (`.*`)

sum( a .* b )

#-
# Ahora, repetiremos el c√°lculo usando la funci√≥n `dot`, de la librer√≠a (est√°ndar) `LinearAlgebra.jl`

using LinearAlgebra

dot(a, b)

# En ambos casos hemos obtenido `0.0`. Suena entonces bastante plausible concluir que $a$ y $b$ son ortogonales; simplemente, no hay raz√≥n por la que debemos esperar otra cosa.

#-
# Una tercer manera de hacer lo mismo es reagrupando las operaciones, es decir, usando la *propiedad asociativa de la suma*. En particular, haremos el c√°lculo calculando primero
f1 = a[1] * b[1] + a[3] * b[3] + a[4] * b[4] + a[6] * b[6]

# seguido de
f2 = a[2] * b[2] + a[5] * b[5]

# Y ahora sumamos ambos t√©rminos, para obtener
f1 + f2

# El resultado obtenido no es cero, y resulta que √©ste es el correcto; esto lo podemos verificar "a mano", o usando precisi√≥n extendida.

# Claramente, el orden en que realizamos las operaciones num√©ricas en la computadora es importante.

# ### Magnitudes relativas

# Consideremos la funci√≥n $$f(x,y) = 333.75 y^6 + x^2(11x^2y^2-y^6-121y^4-2)+5.5y^8+x/(2y)$$. La pregunta es ¬øcu√°l es el valor (¬°correcto!) de $f(x,y)$ en el punto $(77617,33096)$?

# Para responder la pregunta definiremos primero las siguientes funciones:

f‚ÇÅ(x,y) = 333.75*y^6 + x^2*(11*x^2*y^2-y^6-121*y^4-2)

f‚ÇÇ(x,y) = 5.5*y^8

f(x,y) = f‚ÇÅ(x,y) + f‚ÇÇ(x,y) + x/(2*y)

# Procedamos por el camino m√°s sencillo: Evaluemos *directamente* `f(77617, 33096)`

f(77617, 33096)

# Ahora haremos lo mismo usando precisi√≥n extendida
f( big(77617),  big(33096))

#-
# La pregunta es ¬øcu√°l es el resultado correcto?

# Para saber cu√°l es el resultado correcto, evaluaremos por separado las distintas funciones involucradas.

f‚ÇÅ(77617, 33096)

#-
f‚ÇÅ(big(77617), big(33096))

#-
f‚ÇÇ(77617, 33096)

#-
f‚ÇÇ(big(77617), big(33096))

#-
f‚ÇÅ(77617, 33096) + f‚ÇÇ(77617, 33096)

#-
f‚ÇÅ(big(77617), big(33096)) + f‚ÇÇ(big(77617), big(33096))

# La moraleja aqu√≠ es clara: adiciones y substracciones de cantidades cuyo order de magnitud rebasa los d√≠gitos significativos utilizados, pueden llevar a resultados err√≥neos.

#-
# ### Sobre la manera de escribir las funciones

# La idea es, a partir de la gr√°fica del polinomio $p(t)=t^6-6t^5+15t^4-20t^3+15t^2-6t+1$ cerca de 1, estimar el n√∫mero de ceros del polinomio. Recordando el teorema fundamental de la aritm√©tica, a lo mucho debemos tener 6 ra√≠ces reales (ya que el polinomio es de grado 6). Este polinomio, puede ser reescrito como $$q(t) = (t-1)^6$$, siendo ambos polinomios *exactamente* iguales.

# Para graficar usaremos la librer√≠a `Plots.jl` que permite graficar usando `Gnuplot`. Graficaremos $p(t)$ en el intervalo $[0.9951171875, 1.0048828125]$ usando pasos de $1/2^{16}$; la idea de usar estos n√∫meros es que son exactamente representables en la computadora. Usaremos el siguiente c√≥digo

#
Pkg.add("Plots") # Esto agrega la paqueter√≠a Plots.jl

#-
using Plots

#-
p(t) = t^6 - 6t^5 + 15t^4 - 20t^3 + 15t^2 - 6t + 1
q(t) = (t-1)^6

#-
x = 1-5/2^10 : 1/2^16 : 1+5/2^10

#-
plot( x, p.(x), color=:blue, label="p(t)")
plot!(x, q.(x), color=:red, lw=2, label="q(t)")


# El resultado de este ejemplo es que la manera en que uno escribe una funci√≥n es importante num√©ricamente; si bien $p(t)=q(t)$ matem√°ticamente, su implementaci√≥n num√©rica no lo muestra.

#-
# ## N√∫meros en la computadora

# Aqu√≠ veremos las cuestiones b√°sicas de la aritm√©tica implementada en la computadora, qu√© n√∫meros se representan, c√≥mo se guardan y c√≥mo se manipulan, cuestiones que van m√°s all√° del lenguaje concreto que se utiliza. La idea es mostrar por qu√© algunas operaciones dan resultados sencillamente *incorrectos* si los pensamos como n√∫meros reales.

#-
# ### Notaci√≥n posicional
#
# Empezaremos recordando la notaci√≥n posicional para los n√∫meros reales $\mathbb{R}$, en una *base* arbitraria $\beta\ge 2$ (donde $\beta$ es un n√∫mero entero). Cualquier n√∫mero real $x$ puede ser escrito como una cadena infinita de la forma:
# $$
# x = (-1)^\sigma \left( b_n b_{n-1}\dots b_0 . b_{-1} b_{-2} \dots \right)_\beta\ .
# $$
#
# Aqu√≠, $\sigma=\{0,1\}$ sirve para definir el signo de $x$, y $b_n, b_{n-1}, \dots$ son n√∫meros *enteros* tales que $0 \le b_i \le \beta-1$ para todo $i$.

#-
# Claramente, el n√∫mero real $x$ se puede reescribir como:
#
# $$
# \begin{align*}
# x &= (-1)^\sigma \sum_{i=-\infty}^n b_i \,\beta^i\\
#   &= (-1)^\sigma ( b_n \beta^n + b_{n-1} \beta^{n-1} + \dots + b_0 + b_{-1}\beta^{-1} + b_{-2}\beta^{-2} + \dots).
# \end{align*}
# $$
#
# lo que muestra que la notaci√≥n posicional no es √∫nica.

#-
# En la notaci√≥n posicional entonces se utilizan una serie de reglas de conveniencia, esencialmente para evitar redundancias en la representaci√≥n. Por ejemplo, si hay una cola infinita de ceros, √©sta se omite. De igual manera, se omiten los ceros que est√°n antes (a la izquierda) de la parte entera.

# A pesar de esto, la representaci√≥n posicional tiene problemas en toda base $\beta$, dado que hay n√∫meros reales que no tienen una representaci√≥n √∫nica. Por ejemplo, $(3.1415999999\dots)_{10}\,$, con una cola infinita de nueves, es igual a $(3.1416)_{10}\,$. Esta redundancia se puede eliminar si, adem√°s de las reglas de conveniencia anteriores, se a√±ade el requisito de que $0\le b_i \le \beta-2$, para un n√∫mero *infinito* de $i$. Esto es, se eliminan las colas de $\beta-1$ infinitas, a lo mucho permitiendo una cola infinita donde se repite hasta $\beta-2$.

#-
# ### N√∫meros de punto flotante

#-
# Los *n√∫meros de punto flotante* proporcionan una manera conveniente de representar los n√∫meros reales, donde el lugar del punto "decimal", o mejor dicho, el punto en base $\beta$, es importante. Los n√∫meros de punto flotante se representan como:
#
# $$
# x = (-1)^\sigma \, m \times \beta^\varepsilon,
# $$
#
# donde $\sigma$ se relaciona con el signo igual que antes, $m$ es la *mantisa* (o *significante*) y $\varepsilon$ es un exponente.
#
# Para los n√∫meros de punto flotante el lugar del punto en base $\beta$ se fija por convenci√≥n: el punto en base $\beta$ viene despu√©s del primer d√≠gito de $m$,
#
# $$
# m=(b_0.b_{1}b_{2}\dots)_\beta,
# $$
#
# donde los sub√≠ndices de la parte *fraccionaria* de la mantisa (a la derecha del punto) ahora se etiquetan con enteros positivos (antes us√°bamos enteros negativos).

#-
# El conjunto de n√∫meros descritos de esta manera se conoce como los *n√∫meros de punto flotante* en base $\beta$, y a este conjunto se le representa con $\mathbb{F}_\beta$. Igual que antes, $\beta\ge 2$ es un entero, $\sigma=\{0,1\}$, el exponente $\varepsilon$ es alg√∫n entero, $0\leq b_i \leq \beta-1$ para todo $i$, y $0\leq b_i \leq \beta-2$ y para un n√∫mero infinito de $i$, a fin de evitar las colas infinitas de $\beta-1$.

#-
# ### N√∫meros normales y subnormales

#-
# Los n√∫meros de punto flotanto descritos antes introducen una nueva redundancia, por ejemplo, $(42)_{10}$ es igual que $(0.0042)_{10}\times 10^4$ y tambi√©n a $(42000)_{10}\times 10^{-3}$. Para evitar esta redundancia, se pide que la parte entera del n√∫mero consista de un "d√≠gito", $b_0$, que es el d√≠gito que est√° inmediatamente antes del punto "decimal", y que √©ste sea distinto de cero; la √∫nica excepci√≥n es el caso especial $x=0$. Este requisito adicional hace que el exponente $\varepsilon$ sea √∫nico, y de hecho, el m√≠nimo que hace que $b_0$ sea distinto de cero. Los n√∫meros de punto flotante que satisfacen este requisito se llaman *normales* o *normalizados*.

#-
# Sin embargo, dado que la memoria de la computadora es finita, no podemos representar a $\mathbb{R}$ en la computadora, ni tampoco a $\mathbb{F}_\beta$, ya que ambos conjuntos son densos. Es por esto que debemos limitarnos a un subconjunto de n√∫meros que sirvan para aproximar a los n√∫meros reales, siendo √©ste un conjunto *finito*.

#-
# Para definir un conjunto finito, en primer lugar, restringimos el n√∫mero de "d√≠gitos" que representan la mantisa $m$ a un n√∫mero finito, es decir, $m=(b_0.b_1 b_2 \dots b_{p-1})_\beta$; el n√∫mero $p$ se conoce como *precisi√≥n*. Vale la pena notar que la restricci√≥n de que $0\leq b_i \leq \beta-2$ se vuelve irrelevante con precisi√≥n finita, ya que no puede haber colas infinitas de ning√∫n tipo. El conjunto que resulta, que representaremos como $\mathbb{F}_{\beta,p}$, se puede demostrar que es un conjunto contable infinito (es un subconjunto de los racionales), por lo que a√∫n debemos restringirlo m√°s.

#-
# Para obtener un conjunto finito y √∫til en la computadora, fijamos la precisi√≥n $p$ a un valor finito, y adem√°s acotamos (por arriba y abajo) los posibles valores del exponente $\varepsilon$, es decir, $e_- \le \varepsilon \le e_+$. Al conjunto (finito) definido de esta manera lo denotaremos por $\mathbb{F}_{\beta,p}^{e_-, e_+}$.

#-
# La cardinalidad de $\mathbb{F}^{e_-,e_+}_{\beta,p}$, es decir, el n√∫mero de elementos que tiene un conjunto, es
#
# $$
# \#\{\mathbb{F}^{e_-,e_+}_{\beta,p}\} = 1 + 2(e_+-e_-+1)(\beta-1)\beta^{\,p-1}.
# $$

#-
# El valor m√°s peque√±o distinto de cero, $x_\textrm{min}$, se obtiene con la mantisa $(1.0)_\beta$ y el exponente $\varepsilon=e_-$. De manera similar, el valor m√°s grande del conjunto, $x_\textrm{max}$, tiene una mantisa con todos los "d√≠gitos" de la mantisa iguales a $\beta-1$, y con el m√°ximo exponente $\varepsilon=e_+$. Estos valores vienen dados por:
#
# $$
# \begin{align*}
# x_\textrm{min} &= (1.0)_\beta \beta^{\,e_-} = \beta^{\,e_-},\\
# x_\textrm{max} &= ( b_m. b_m\dots b_m)_\beta \beta^{\,e_+} = (\beta-1)\beta^{\,e_+}\sum_{i=0}^{p-1}\beta^{\,-i} \\
#        &= \beta^{\,e_++1-p}(\beta^{\,p}-1).
# \end{align*}
# $$

#-
# Para ilustrar la construcci√≥n, usaremos como ejemplo $\mathbb{F}^{-1,2}_{2,3}$. De las expresiones anteriores tenemos que $\#\{\mathbb{F}^{-1,2}_{2,3}\}=33$, $x_\textrm{min}=0.5$ y $x_\textrm{max}=7$. En este caso podemos obtener expl√≠citamente todos los elementos de este conjunto.

#-
#Generamos todos los n√∫meros normales dados e_-, e_+, prec, para $\beta=2$
ùîΩ = Float64[0.0]
b‚ÇÄ = 1.0
for Œµ = -1:2
	for b‚ÇÅ=0:1
		for b‚ÇÇ=0:1
			x = (b‚ÇÄ + b‚ÇÅ*(2.0)^(-1) + b‚ÇÇ*(2.0)^(-2))*2.0^Œµ
			push!(ùîΩ, x, -x)
		end
	end
end
sort!(ùîΩ)

#-
# Una representaci√≥n gr√°fica de este conjunto se observa en la siguiente gr√°fica.

#-
#Usamos Gaston.jl (que es muy parecido a gnuplot) para representar este conjunto
xmax = maximum(ùîΩ)
plot(size=(600,100))
plot!( ùîΩ, zero.(ùîΩ), yerr=0.5, markerstrokecolor=:auto, lw=2,
    xlims=(-xmax-0.5, xmax+0.5), ylims=(-1,1),
    xticks=collect(-xmax:xmax), yticks=:none,
    label=:none)
plot!( [-xmax-0.5, xmax+0.5], zeros(2), color=:black, lw=2,
    label=:none)

#-
# Como se puede observar de la gr√°fica, la distancia entre puntos vecinos de $\mathbb{F}_{2,3}^{-1,2}$ es constante por segmentos. Estos segmentos est√°n separados por potencias consecutivas de $\beta$ obtenidas al variar $\varepsilon$, y la distancia entre n√∫meros consecutivos disminuye a medida que el exponente $\varepsilon$ disminuye, excepto alrededor del $0$.
# Esta "propiedad" hace que haya una gran p√©rdida de *exactitud* cuando se aproximan n√∫meros muy peque√±os. De hecho, tambi√©n hace que ciertas propiedades aritm√©ticas no se cumplan en la computadora.
#
# Una manera de evitar el *hueco* tan amplio alrededor del cero, al menos de manera parcial, es permitiendo tener n√∫meros que no son *normales*. Los n√∫meros de punto flotante en $\mathbb{F}_{\beta,p}^{e_-,e_+}$ con $b_0=0$ y $\varepsilon=e_-$ se llaman n√∫meros *subnormales*.

#-
#Generamos todos los n√∫meros subnormales para beta=2 y se los a√±adimos a los normales
sùîΩ = Float64[]
Œµ = -1
b‚ÇÄ = 0.0
for b‚ÇÅ=0:1
	for b‚ÇÇ=0:1
        b‚ÇÅ == b‚ÇÇ == 0 && continue # no repetimos al 0
		x = ( b‚ÇÄ + b‚ÇÅ*(2.0)^(-1) + b‚ÇÇ*(2.0)^(-2))*2.0^Œµ
		push!(sùîΩ, x, -x)
	end
end
sort!(sùîΩ)

#-
#Representaci√≥n gr√°fica de sùîΩ
plot(size=(600,100))
plot!( ùîΩ, zero.(ùîΩ), yerr=0.5, markerstrokecolor=:auto, lw=2,
    xlims=(-xmax-0.5, xmax+0.5), ylims=(-1,1),
    xticks=collect(-xmax:xmax), yticks=:none,
    label=:none)
plot!( sùîΩ, zero.(sùîΩ), yerr=0.5, markerstrokecolor=:auto, label=:none)
plot!( [-xmax-0.5, xmax+0.5], zeros(2), color=:black, lw=2,
    label=:none)


#-
# Incluir a los n√∫meros subnormales junto con los n√∫meros normales (de punto flotante) permite que haya una transici√≥n gradual hacia $0$. Una propiedad importante que se puede demostrar es que la representaci√≥n de todos los n√∫meros (normales y subnormales) distintos de cero es √∫nica.
#
# El conjunto $\mathbb{F}_{\beta,p}^{e_-,e_+}$, entendido como los n√∫meros de punto flotante normales y subnormales, son esencialmente los que la computadora utiliza.

#-
# S√≥lo nos falta entonces establecer la manera de mapear $\mathbb{R}$ a $\mathbb{F}$."

#-
# ## Redondeo

#-
# El mapeo que se encarga de pasar de $\mathbb{R}$ a $\mathbb{F}$ es el *redondeo*. Obviamente, este mapeo no puede ser invertible.

#-
# Antes de definir el mapeo, o de hecho *los mapeos* ya que no es √∫nico, es √∫til extender el dominio y el rango de los conjuntos en $\mathbb{R}^*=\mathbb{R}\cup\{-\infty,\infty\}$ y de la misma manera $\mathbb{F}^*=\mathbb{F}\cup\{-\infty,\infty\}$. Esta extensi√≥n permite representar los n√∫meros que son m√°s grandes que el mayor de los n√∫meros de punto flotante, $x_\textrm{max}$.

#-
# Pedimos que un mapeo de redondeo $\bigcirc$ satisfaga las siguientes dos propiedades:
#
# - (R1) $x\in\mathbb{F}^* \Rightarrow \bigcirc(x)=x,$
# - (R2) $x,y\in\mathbb{F}^*$ y $x\le y$ $\Rightarrow \bigcirc(x)\le \bigcirc(y).$
#
# Con estas propiedades se puede demostrar que el redondeo es de *calidad m√°xima*: el interior del intervalo que se puede definir con $x$ y $\bigcirc(x)$ no contiene puntos de $\mathbb{F}^*$.

#-
# ### Redondeo a cero

#-
# La manera m√°s sencilla de redondear es el modo conocido como redondeo a cero. Este modo es equivalente al truncamiento, es decir, a omitir todos los "d√≠gitos" de la mantisa que est√°n m√°s all√° de la precisi√≥n. De esta manera definimos
#
# ```math
# \begin{equation*}
# \square_z(x) = \textrm{sign}(x) \max\left(y\in\mathbb{F}^*: y\le |x|\right).
# \end{equation*}
# ```
#
# En este caso, si escribimos $x=(-1)^\sigma(b_0.b_1b_2\dots)_\beta\,\beta^e$, entonces el redondeo a cero corresponde a $\square_z(x) = (-1)^\sigma(b_0.b_1b_2\dots b_{p-1})_\beta\,\beta^e$.
#
# El redondeo a cero es una funci√≥n impar.

#-
# ### Redondeo dirigido

#-
# Podemos definir dos modos de redondeo dirigido:
# $$
# \begin{align*}
# x\in\mathbb{R}^* &\Rightarrow \bigcirc(x)\le x, &(a)\\
# x\in\mathbb{R}^* &\Rightarrow \bigcirc(x)\ge x. &(b)\\
# \end{align*}
# $$

#-
# El redondeo que satisface $(a)$ se llama *redondeo hacia menos infinito* (o redondeo hacia abajo), y el que satisface $(b)$ es el *redondeo hacia m√°s infinito* (redondeo hacia arriba). Estos redondeos se definen de manera formal como:
# $$
# \begin{align*}
# \bigtriangledown(x) &= \max\left(y\in\mathbb{F}^*:y\le x\right),\\
# \bigtriangleup(x) &= \max\left(y\in\mathbb{F}^*:y\ge x\right).  \\
# \end{align*}
# $$
#
# En este caso, el intervalo $[\bigtriangledown(x), \bigtriangleup(x)]$ es de m√°xima calidad.
#
# Estos redondeos satisfacen la propiedad:
# $$
# \bigtriangledown(x) = -\bigtriangleup(-x),
# $$
# lo que permite obtener un redondeo en t√©rminos del otro.

#-
# ### Redondeo (al par) m√°s cercano

#-
# Tanto el redondeo a cero como los redondeos dirigidos mapean el interior de un intervalo formado por dos puntos consecutivos de $\mathbb{F}^*$ a un punto de $\mathbb{F}^*$. Esto significa que el error que se comete a trav√©s de estos tipos de redondeo es del orden de la longitud del intervalo definido por los dos puntos consecutivos de $\mathbb{F}^*$, i.e., la longitud de $[\bigtriangledown(x),\bigtriangleup(x)]$. Una forma de redondeo que resulta en errores menores es el *redondeo al punto flotante m√°s cercano*.

#-
# Todo punto $x\in\mathbb{R}^*$ puede ser acotado en $\mathbb{F}^*$ usando el intervalo $[\bigtriangledown(x),\bigtriangleup(x)]$, es decir
# $$
# \bigtriangledown(x) \le x \le \bigtriangleup(x).
# $$
#
# La idea entonces del redondeo al punto flotante m√°s cercano es la siguiente: Para todo punto $|x| \le x_\textrm{max}$, definimos el punto medio del intervalo de puntos de $\mathbb{F}^*$ que los acota como $\mu=(\bigtriangledown(x) + \bigtriangleup(x))/2$; si $|x| > x_\textrm{max}$ entonces definimos $\mu=\textrm{sign}(x) |x_\textrm{max}|$. Entonces, el redondeo al punto flotante m√°s cercano regresa $\bigtriangledown(x)$ si $x < \mu$ y $\bigtriangleup(x)$ si $x > \mu$. En el caso en que $x=\mu$, las distintas maneras de resolver el empate definen los distintos modos del redondeo al punto m√°s cercano.

#-
# Una manera para no tener *sesgo* en el error, es decir, que el error est√© centrado alrededor de cero es usando lo que se llama el *redondeo al par m√°s cercano*, y que denotaremos como $\square$. Esta manera de redondear es la que se encuentra de manera m√°s com√∫n.
#
# Escribimos la mantisa de los n√∫meros de punto flotante que acotan $x$ como $(a_0.a_1a_2\dots a_{p-1})_\beta$ y $(b_0.b_1b_2\dots b_{p-1})_\beta$. Entonces, si $x\notin \mathbb{F}^*$, definimos este modo como
#
# $$
# \begin{align*}
# x>0 \Rightarrow \square(x) &=
# \begin{cases}
# \bigtriangledown(x), \textrm{si } x\in[\bigtriangledown(x),\mu), \textrm{ o si } x=\mu \textrm{ y } a_{p-1} \textrm{ es par},\\
# \bigtriangleup(x), \textrm{si } x\in(\mu,\bigtriangleup(x)], \textrm{ o si } x=\mu \textrm{ y } b_{p-1} \textrm{ es par},\\
# \end{cases}\\
# x<0 \Rightarrow \square(x) &= - \square(-x).
# \end{align*}
# $$
#
# Un punto importante que vale la pena recalcar es que si $|x|>x_\textrm{max}$, el redondeo al punto flotante par m√°s cercano da como resultado $\square(x) = \text{sign}(x)\infty$, que no necesariamente es el punto flotante m√°s cercano.

#-
# De manera similar uno puede definir el redondeo al punto flotante *impar* m√°s cercano.
